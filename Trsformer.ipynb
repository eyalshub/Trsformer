{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pytorch as nn\n",
        "import numpy as np\n",
        "import lightning as L\n",
        "import copy as copy"
      ],
      "metadata": {
        "id": "ZDE7_CAXpjVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sublayer"
      ],
      "metadata": {
        "id": "TbxgUdPdpNA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class sublayerconnection(nn.Modile):\n",
        "  def __init__(self.sice.dropoat):\n",
        "    super(sublayerconnection,self).__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self,x,sublayer):\n",
        "    \"Apply residual connection to any sublayer function that maintains the same size\"\n",
        "    return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n"
      ],
      "metadata": {
        "id": "w6Jgx2cQpMZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#clones"
      ],
      "metadata": {
        "id": "cGxRaarDp8s9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKTdMucypFLn"
      },
      "outputs": [],
      "source": [
        "def clones(module,N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#layer normaliztion"
      ],
      "metadata": {
        "id": "90-Xe1zQqFw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n"
      ],
      "metadata": {
        "id": "JMa_A_OJqK-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoder"
      ],
      "metadata": {
        "id": "7CtNVaOtqsAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.module):\n",
        "    def __init__(self, size, self_attn,src_attn,feed_forward,dropout):\n",
        "      supor(DecoderLayer,self).__init__()\n",
        "      self.size = size\n",
        "      self.self_attn = self_attn\n",
        "      self.src_attn = src_attn\n",
        "      self.feed_forward = feed_forward\n",
        "      self.sublayer = clones(sublayerconnection(size,dropout),3)\n",
        "\n",
        "    def forward(self,x,memory,src_mask,tgt_mask):\n",
        "      m = memory\n",
        "      x = self.sublayer[0](x,lambda x:self.self_attn(x,x,x,tgt_mask))\n",
        "      x = self.sublayer[1](x. lambda x: self.src_attn(x,m,m,src_mask))\n",
        "      return self.sublayer[2](x,self.feed_forward)\n",
        "\n",
        "class Decoder(nn.module):\n",
        "  def __init__(sef,layer,N):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.layers = clones(layer,N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self,x,memory,src_mask,tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x,memory,src_mask,tgt_mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "c-wGLlzjqt37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder"
      ],
      "metadata": {
        "id": "g3QIiNecrwFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.module):\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self.attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(sublayerconnection(size,droput),2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "      x = self.sublayer[0](x,lambda x:self.self_attn(x,x,x,mask))\n",
        "      return self.sublayer[1](x,self.feed_forward)\n",
        "\n",
        "class Encoder(nn.module):\n",
        "  def __init__(self,layer,N):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.layers = clones(layer,N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x,mask)\n",
        "     return self.norm(x)\n"
      ],
      "metadata": {
        "id": "4yMEGzharyhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Masking future values"
      ],
      "metadata": {
        "id": "M9Ft72fsto8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "  attn_shape = (1,size,size)\n",
        "  subsequent_mask = np.triu(np.ones(attn_shape),k=1).astype('unit8')\n",
        "  return torch.from_numpy(subsequent_mask) == 0"
      ],
      "metadata": {
        "id": "iU4SGZKut0t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder - Decoder"
      ],
      "metadata": {
        "id": "pz6YVU-gu_rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(L.LightningModule):\n",
        "  def __init__(self,encoder,decoder,soarce_embed,target_embed,generator,criterion):\n",
        "    super(EncoderDecoder,self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = soarce_embed\n",
        "    self.tgt_embed = target_embed\n",
        "    self.generator = generator\n",
        "    self.criterion =criterion\n",
        "\n",
        "  def encode(self,sre,src_mask):\n",
        "    return self.encoder(self.src_embed(src),src_mask)\n",
        "\n",
        "  def decode(self,memory,src_mask,tgt,tgt_mask):\n",
        "    return self.decoder(self.tgt_embed(tgt),memory,src_mask,tgt_mask)\n",
        "\n",
        "  def forward(self,src,tgt,src_mask,tgt_mask):\n",
        "    return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
        "\n",
        "  # def generator_score(self,logits):\n",
        "  #   return F.log_softmax(logits,dim=-1"
      ],
      "metadata": {
        "id": "Sq56i3KEvDZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention"
      ],
      "metadata": {
        "id": "rLcSsaY_wyZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query,key,value,mask=None,dropout=None):\n",
        "  d_k = query.size(-1)\n",
        "  scores = torch.matmul(query,key.transpose(-1,-2))/math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask==0,-1e9)\n",
        "  p_attn = F.softmax(scores,dim=-1)\n",
        "  if dropout is not None:\n",
        "    p_attn = dropout(p_attn)\n",
        "  return torch.matmul(p_attn,value),p_attn"
      ],
      "metadata": {
        "id": "qXRki2h4xhob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi Head Attention"
      ],
      "metadata": {
        "id": "GuXnQbG6xWPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "  def __init__(self,h,d_model,dropout=0.1):\n",
        "    super(MultiHeadedAttention,self).__init__()\n",
        "    assert d_model % h == 0\n",
        "    self.d_k = d_model // h\n",
        "    self.h = h\n",
        "    self.linears = clones(nn.Linear(d_model,d_model),4)\n",
        "    self.attn = None\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,query,key,value,mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask.unsqueeze(1)\n",
        "    nbatches = query.size(0)\n",
        "    query,key,value = [l(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2) for l,x in zip(self.linears,(query,key,value))]\n",
        "    x , self.attn = attention(query,key,value,mask=mask,dropout=self.dropout)\n",
        "    x = x.transpose(1,2).contiguous().view(nbatches,-1,self.h*self.d_k)\n",
        "    return self.linears[-1](x)\n",
        "\n"
      ],
      "metadata": {
        "id": "T-x7zTBkxaKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#position- wise feed forward"
      ],
      "metadata": {
        "id": "wekrhIZ4yKzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout=0.1):\n",
        "    super(PositionwiseFeedForward,self).__init__()\n",
        "    self.w_1 = nn.Linear(d_model,d_ff)\n",
        "    self.w_2 = nn.Linear(d_ff,d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "metadata": {
        "id": "ujLXk1uzyQt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embeddings"
      ],
      "metadata": {
        "id": "ouNhbkn34GfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)  # look up matrix\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "ULab-PNQ4IjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#positional encoding"
      ],
      "metadata": {
        "id": "Emgj0C6ZyhWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class position_encoding(nn.Module):\n",
        "  def __init__(self,d_model,dropout,max_len=1000):\n",
        "    super(position_encoding,self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    pe = torch.zeros(max_len,d_model)\n",
        "    position = torch.arange(0,max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0,d_model,2)*-(math.log(10000.0)/d_model))\n",
        "    pe[::2]=torch.sin(position*div_term)\n",
        "    pe[1::2]=torch.cos(position*div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe',pe)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + Variable(self.pe[:,:x.size(1)],requires_grad=False)\n",
        "    return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "wHoI3V41ylb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Label Smoothing"
      ],
      "metadata": {
        "id": "iMfRvEjmzNVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "  def __init__(self,size,padding_idx,smoothing=0.0):\n",
        "    super(LabelSmoothing,self).__init__()\n",
        "    self.criterion = nn.KLDivLoss(size_average=False)\n",
        "    self.padding_idx = padding_idx\n",
        "    self.confidence = 1.0 - smoothing\n",
        "    self.smoothing = smoothing\n",
        "    self.size = size\n",
        "    self.true_dist = None\n",
        "\n",
        "  def forward(self,x,target):\n",
        "    assert x.size(1) == self.size\n",
        "    true_dist = x.data.clone()\n",
        "    true_dist.fill_(self.smoothing/(self.size-2))\n",
        "    turt_dist.scatter_(1,target.data.unsqueeze(1),self.confidence)\n",
        "    true_dist[:,self.padding_idx] = 0\n",
        "    if mask.dim() > 0:\n",
        "      true_dist.index_fill_(0,mask.squeeze(),0.0)\n",
        "    self.true_dist = true_dist\n",
        "    return self.criterion(x,nn.parameter(true_dist,requires_grad=False))"
      ],
      "metadata": {
        "id": "9Ur5BKXzzRRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Noam Schedaler"
      ],
      "metadata": {
        "id": "9wGQXNjH0aSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoamScheduler(Lr_scheduler):\n",
        "  def __init__(self,optimizer,model_size,fector,warmap,last_epoch =1):\n",
        "    self.model_size = model_size\n",
        "    self.fector = fector\n",
        "    self.warmap = warmap\n",
        "    super(NoamScheduler,self).__init__(optimizer,last_epoch)\n",
        "\n",
        "  def get_lr(self):\n",
        "    step = self.last_epoch + 1\n",
        "    lr = self.fector *(self.model_size**(-0.5)*min(step**(-0.5),step*self.warmap**(-1.5))\n",
        "    return [lr for _ in self.optimizer.param_groups]"
      ],
      "metadata": {
        "id": "0kYoXhDD0eB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create our model"
      ],
      "metadata": {
        "id": "gRU_nmWi1dmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(src_vecab, tgt_vocab,N=6,d_model=512,d_ff=1024,h=8,droput=0.1,criterion = nn.BCELoss):\n",
        "  c = copy.deepcopy\n",
        "  attn = MultiHeadedAttention(h,d_model)\n",
        "  ff = PositionwiseFeedForward(d_model,d_ff,dropout)\n",
        "  position = position_encoding(d_model,dropout)\n",
        "  model = EncoderDecoder(Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n",
        "                         Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n",
        "                         nn.sequential(Embeddings(d_model,src_vocab),c(position)),\n",
        "                         nn.sequential(Embeddings(d_model,tgt_vocab),c(position)),\n",
        "                         Generator(d_model,tgt_vocab),criterion)\n",
        "  for p in model.parameters():\n",
        "    if p.dim()>1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "  return model"
      ],
      "metadata": {
        "id": "Qy_JwdzGww5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-useFcJ14aQb"
      }
    }
  ]
}